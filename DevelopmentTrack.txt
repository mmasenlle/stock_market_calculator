Calculinator Development Track
==============================

Thu Apr  1 12:04:47 CEST 2010
 Research and startup (informal)
---------------------------------
 -Sketch the architecture
 -Build working repository/environment
 -Brief research about few subjects: data feed sources, db, ...


Thu Apr  1 19:13:40 CEST 2010
 Data feed program I
--------------------
 -Start ccltor_feeder
  +Main loop
  +Check date and time
  +Fetch page
  +Parse page
  +Basic configuration
Comments:
The parse will be split in two parts: one that will be handled with an
external program/script, and other built-in. This way we can have separate
parser for different sources. The first parser will translate the data to a
well known xml syntax that the built-in parser will handle. The external
parsers will be created when needed, and will be expecified in the
configuration with the url.


Wed Apr  7 23:08:53 CEST 2010
 DB setup
----------
 -Install the DBMS, create a db, users, etc.
 -Sketch a DB schema for the system.
 -Create scripts to build the first approach of the schema.


Sat Apr 10 20:20:05 CEST 2010
 Data feed program II
---------------------
 -Continue ccltor_feeder
  +Sending of data to the db
  +An observer/notifier mechanism
Comments:
For the intercommunications of the tasks, sending events and so on, there
will be a IC subsystem in the folder: ccltor/ic. The messages Event,
MsgRegister, Raw, etc, will be binded in classes.
For the db access, the classes to isolate the DBMS will be in the ccltor/db
folder.


Thu Apr 15 23:14:35 CEST 2010
 Validate feeder
-----------------
 -Debug and fix the corner cases of the program
 -Do a good test in production


Thu Apr 15 23:19:39 CEST 2010
 Chart I
---------
 -Do a program to draw simple charts
  +Read simple options (value, day, ...)
  +Get the date for price from db
  +Print the points
Comments:
Working, but is done in opengl and is a bit poor in text information. For the
next iteration on charts, better use a specific library (i.e. gnuplot).


Sun Apr 18 22:37:00 CEST 2010
 Running control I
------------------
 -Start a program to send messages to processes
  +First configuration (peers, event numbers)
  +Send and finish
Comments:
Maybe add in the future support to receive messages (ack and so on).
Maybe add parameters more human friendly instead of event numbers.


Wed Apr 21 23:38:43 CEST 2010
 Cruncher concept probes
-------------------------
 -Program a cruncher manager
  +Load plugins as shared libraries
  +Create a thread/process for them (try to use clone)
 -Program a plugin test
Comments: the plugin is load as shared library and the process is clone for
each. A test and the statistics process are done. The notifications,
internally and with others needs to be worked better.


Wed Apr 28 18:39:29 CEST 2010
 Output I (Chart second iteration)
----------------------------------
 -Port/rework chart to a task called output
  +Use get data adapted to the new db interfaces
  +Add the ability to handle different front ends
 -Add a stub frontend to output html tables
 -Port the chart to use gnuplot instead of opengl
  +Add more titles, labels, and so forth, than in the opengl version


Fri Apr 30 22:24:25 CEST 2010
 Research in real calculations
-------------------------------
 -Bibliography revision about indicators
 -Think of a simple real world calculation that could be implemented
Comments: a trends called cruncher plugin has been implemented. It calculates
very simple trend values (pivot points, resistance and support values, moving
averages, etc). Now there is the need to understand and be able to interpret
the indicators and maybe calculate in the following days the matches of the guesses.


Fri May 14 20:15:13 CEST 2010
 DBCache debug and testing
---------------------------
Comments: It's been some work here and there. Most of all, the output task has
now support for configuring and displaying multiple data; the open and close
values has been passed to statistics tables, trends has winned two more values
(ad and obv). The cruncher manager supports inter plugin messages and a new
event (statistics updated) has been created. Other major addition is the
DBCache in the cruncher manager. This cache's goal is to reduce accesses to
the dbms. For now there is the need of a new connection to the db but with hope
the accesses are going to be reduced.
 -Update schemas of statistics and trends tables
 -Test cruncher cache and plugins
 -Try to fill the tables with correct data
 -Test the improvements due to the cache


Sat Jun  5 17:05:09 CEST 2010
 Wealth tables and cruncher plugin
-----------------------------------
 -Create tables wealth_* for existing all day values
 -Implement a cruncher plugin to calculate the sum of each value


Sun Jun  6 21:51:17 CEST 2010
 Regression cruncher
---------------------
 -Some research about regression analysis and methods
 -Implement something as a first approach
Comments: three step approach will be use (forecasting methods)
1- First a simple n-order interpolation on P
2- Then try a multivar interpolation on vars open, min, mean, max, close, mean_volume
and close_volume for example.
3- Finally try to calculate a regression function using numerical methods for
least square. Maybe use the same variables as before in order to obtain min
max values of several future days.


Tue Jun  8 20:44:35 CEST 2010
 Scheduler
-----------
 -Implement a scheduler task
  +Configuration with tasks where task has a day mask, a time and a command to
  execute.
  +The process will run the commands at the times on the enabled days.
Comments: the commands are configured only by file and may have a day mask, a
starting time and an interval, despite the command itself. The command is a
string line that is converted to an array of arguments, splitted by blanks. If
the interval is 0, the command is run every day that is not masked at its
starting time. If starting time is 0, starting time is 00:00:00. If interval
is negative, the command is executed only once. If interval is positive, the
starting time is used to start the secuence. Mask days are skipped but the
secuence is not restarted.


Tue Jun  8 20:53:08 CEST 2010
 Simple interpolator
---------------------
 -Cruncher plugin to calculate and evaluate a 20-order interpolation function
 of P.
  +Define a table to hold the parameters and the forecasted value of P for
  each day and value.
  +Follow the pattern of current crunchers, going from today back calculating
  and filling the table.
Comments: The coefficients of the polynomial are estimated with an iterative
method that is very convenient but not accurate. Anyway, the model is very
very bad, although as a concept of probe has been a good exercise. This plugin is
not worth of maintaining, but it can be used very easily as template for the next
step of the forecasters. At the end, the multivar interpolation is going to be the
same as simple one, but there is room to improve and investigate. The most
important is to create a good model, try to figure out the best variables and
the order of each one. Then use a more sofisticated method to estimate the
linear system.


Wed Jun 16 23:17:58 CEST 2010
 Support no unique value codes
-------------------------------
 -Study how to support different codes for the same value from different
 sources.
  +Redesign db schema if needed
  +Do a script or program to convert the data
Comments: Now there are to id's, one for the source (web page), and other for
the ccltor system. The feeds are identified by the source id, but the rest of
the calculations are based on the ccltor id (statistics, trends, etc). The
insertion of the feeds are quite the same, but the retrieve of the values are
done with a join between feeder_value_ids and feeder_value_data because the
code used may imply several codes in the table of the data. The binding
between the codes ought to be done manually. There are also scripts for
creating all the tables with the new model, for coping the feeder data to the
new tables and so on. A script with the binding of the codes is under
construction, but even without it the system is by now operative.


Sun Jun 27 20:52:07 CEST 2010
 Interpolator II (step 2 towards regression)
---------------------------------------------
 -Change interpolation plugin to a more interesting and multivar model
  +Create a data schema to support more generic interpolation equations
  +Modify interpolation calculate function to do:
   * Obtein the coefficients of the interpolator to extrapolate min from 5
     days of min,max,close,mean_vol and close_vol
   * The same to extrapolate max
  +Extrapolate only for one day unless force_until
 -Improve matrix and equation helpers
  +Implement det using triangulization
  +Implement an efficient diagonalization
  +Try to use a more sophisticated approach in equation::solve (maybe CG)
 -Study and test direct and iterative methods to obtain the coefficients
Comments: as a concept of probe is quite finished. The system is solved using
LU factorization and the model to use is open to several vars and exponent and
time orders. The issues that remain are: the results obtained are bad (there
is few data yet) and the data generated for the coefficients is huge; is worth
storing ? And how much days calculate forward or backward with each obtained
equation ?
Anyway, there is time to think about it because it would be good to feed much
more data to run the regression.


Sat Jul 10 22:46:55 CEST 2010
 Regression I (not the real one)
----------------------------------
 -Create a cruncher plugin to calculate one regression function (not for each
 value), using interpolator as a base:
  +Construct the matrix from all the values filtering by count > m
  +Calculate the coefficient vectors based on the model using the required data
  from the matrix and going backward with a certain overlap on the matrix's
  rows, maybe 75%.
  +Calculate the squared errors for each coefficient vector for the whole set
  of data.
  +Apply and evolution algorithm on the vectors to improve the best function
  +Evaluate the function for the values, going back if force_until
Comments: it's been done much more easily, using AtAu=Atb. The model is the
same as in interpolator, and can be adjusted in the future with the availability 
of the data. It is done by each value, to avoid the normalization of the data.
Seems to work quite well and now is time to run it everyday and check the guessed
values.


